{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johntanas/it1244project/blob/main/code/model_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5179ea71",
      "metadata": {
        "id": "5179ea71"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder,OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "gwWC421NqpxW",
      "metadata": {
        "id": "gwWC421NqpxW"
      },
      "outputs": [],
      "source": [
        "n_repeats=5\n",
        "n_splits=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "XJlCQwWJEl2_",
      "metadata": {
        "id": "XJlCQwWJEl2_"
      },
      "outputs": [],
      "source": [
        "path_to_max_label=\"../content/max_label.csv\"\n",
        "path_to_app=\"../content/cleaned_application.csv\"\n",
        "cleaned_app_df=pd.read_csv(path_to_app,index_col=0)\n",
        "max_df=pd.read_csv(path_to_max_label,index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "DoxeFDv6Mgj8",
      "metadata": {
        "id": "DoxeFDv6Mgj8"
      },
      "outputs": [],
      "source": [
        "dep_var=\"status\"\n",
        "random_state=42"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LU5wQZdFPNoR",
      "metadata": {
        "id": "LU5wQZdFPNoR"
      },
      "source": [
        "preprocessing the same as other model notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "zieu69u9S3r9",
      "metadata": {
        "id": "zieu69u9S3r9"
      },
      "outputs": [],
      "source": [
        "order=cleaned_app_df.groupby(\"job\")[\"income\"].median().sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "Vq37hbF7TC0v",
      "metadata": {
        "id": "Vq37hbF7TC0v"
      },
      "outputs": [],
      "source": [
        "cleaned_app_df[\"job\"]=cleaned_app_df[\"job\"].replace(order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ZEu51Hh-LyC5",
      "metadata": {
        "id": "ZEu51Hh-LyC5"
      },
      "outputs": [],
      "source": [
        "train_df=max_df.merge(cleaned_app_df,how=\"inner\",on=\"id\").drop(\"id\",axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "JqwrdVI-75AF",
      "metadata": {
        "id": "JqwrdVI-75AF"
      },
      "outputs": [],
      "source": [
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer=OneHotEncoder()\n",
        "ord_transformer=OrdinalEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "V03vSX3e8BDr",
      "metadata": {
        "id": "V03vSX3e8BDr"
      },
      "outputs": [],
      "source": [
        "X,y=train_df.drop(columns=dep_var),train_df[dep_var]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "PxZXExPK77oD",
      "metadata": {
        "id": "PxZXExPK77oD"
      },
      "outputs": [],
      "source": [
        "categorical_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and \n",
        "                        X[cname].dtype == \"object\"]\n",
        "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "GUdPAjpk76qY",
      "metadata": {
        "id": "GUdPAjpk76qY"
      },
      "outputs": [],
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols),\n",
        "        ('ord',ord_transformer,['job'])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "sBxqujKg8FDR",
      "metadata": {
        "id": "sBxqujKg8FDR"
      },
      "outputs": [],
      "source": [
        "xs=preprocessor.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "gpCTr2Oh_t_Q",
      "metadata": {
        "id": "gpCTr2Oh_t_Q"
      },
      "outputs": [],
      "source": [
        "score_df=pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pvixVAlrN2nf",
      "metadata": {
        "id": "pvixVAlrN2nf"
      },
      "source": [
        "cv same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "Uk6kEisNN2AP",
      "metadata": {
        "id": "Uk6kEisNN2AP"
      },
      "outputs": [],
      "source": [
        "cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "nQxbNZZ4OAlw",
      "metadata": {
        "id": "nQxbNZZ4OAlw"
      },
      "outputs": [],
      "source": [
        "# dataframe of scores to be converted into plot later\n",
        "score_df=pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1ZfWWT07KV_s",
      "metadata": {
        "id": "1ZfWWT07KV_s"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "RX72U7191Fyd",
      "metadata": {
        "id": "RX72U7191Fyd"
      },
      "outputs": [],
      "source": [
        "def scheduler(epoch, lr):\n",
        "          if epoch < 10:\n",
        "            return lr\n",
        "          else:\n",
        "            return lr * tf.math.exp(-0.1)\n",
        "schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "# montior is the loss, not metric, patience is personal heuristic\n",
        "earlystop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
        "callbacks=[schedule,earlystop]\n",
        "#Adam > SGD\n",
        "opt = tf.optimizers.Adam(learning_rate=0.001)\n",
        "#Use BinaryFocalCrossentropy instead of BinaryCrossentropy\n",
        "loss= tf.keras.losses.BinaryFocalCrossentropy()\n",
        "# we can use AUC and PR metric\n",
        "metrics = [tf.keras.metrics.AUC(curve='ROC')]\n",
        "def createModel(loss,optimizer,metrics):\n",
        "    model= keras.Sequential([\n",
        "      keras.layers.Dense(1024,activation=\"relu\"),\n",
        "      keras.layers.Dropout(0.2),\n",
        "      keras.layers.Dense(512,activation=\"relu\"),\n",
        "      keras.layers.Dropout(0.2),\n",
        "      keras.layers.Dense(256,activation=\"relu\"),\n",
        "      keras.layers.Dense(128,activation=\"relu\"),\n",
        "      keras.layers.Dense(1,activation=\"sigmoid\")])\n",
        "    model.compile(loss=loss,optimizer=opt, metrics=metrics)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "AzogFSgyKUaT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "AzogFSgyKUaT",
        "outputId": "12956347-44af-45cd-a307-a115d5bf4894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "177/456 [==========>...................] - ETA: 4s - loss: 0.1756 - auc_1: 0.5017"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-6ebdc7d673c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcreateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mclass_w\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "bestscore=0\n",
        "bestmodel,bestmodel_val_data=None,None\n",
        "bestmodel_val_data=None\n",
        "scores=[]\n",
        "for train_index, test_index in cv.split(xs,y):\n",
        "      X_train = xs[train_index]\n",
        "      y_train = y[train_index]\n",
        "      X_test = xs[test_index]\n",
        "      y_test = y[test_index]\n",
        "      model= createModel(loss=loss,optimizer=opt, metrics=metrics)\n",
        "      schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "      earlystop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
        "      callbacks=[schedule,earlystop]\n",
        "      class_w= dict(zip(np.unique(y_train),compute_class_weight(\"balanced\",classes=np.unique(y_train),y=y_train)))\n",
        "      history=model.fit(X_train, y_train,batch_size=64, epochs=100,validation_data=(X_test,y_test),verbose=1,callbacks=callbacks,class_weight=class_w)\n",
        "      y_pred = model.predict(X_test,verbose=0)\n",
        "      score=roc_auc_score(y_test,y_pred)\n",
        "      scores.append(score)\n",
        "      if score>bestscore:\n",
        "            bestmodel=model\n",
        "            bestscore,bestmodel_val_data=score,[y_test,X_test]\n",
        "            print(f\"NN with best score of {score}\")\n",
        "score_df[\"NN\"]=scores\n",
        "print(f\"NN with avg score of {np.median(np.array(scores))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K7bwex0kS2Yi",
      "metadata": {
        "id": "K7bwex0kS2Yi"
      },
      "outputs": [],
      "source": [
        "y_test,X_test=bestmodel_val_data\n",
        "view=pd.DataFrame([model.predict(X_test).reshape(-1),y_test.values]).T\n",
        "view.columns=[\"preds\",\"ground_truth\"]\n",
        "view.boxplot(by='ground_truth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PxwPQbYOF-qb",
      "metadata": {
        "id": "PxwPQbYOF-qb"
      },
      "outputs": [],
      "source": [
        "# classes are balanced ,we use BCE\n",
        "loss= keras.losses.BinaryCrossentropy()\n",
        "earlystop=keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True)\n",
        "callbacks=[schedule,earlystop]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05d6081d",
      "metadata": {
        "id": "05d6081d"
      },
      "outputs": [],
      "source": [
        "score_df.to_csv(\"../content/score_nn.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "aQEChnN0GV2r",
      "metadata": {
        "id": "aQEChnN0GV2r"
      },
      "outputs": [],
      "source": [
        "score_df_smote=pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "zaCI_Np_G5Zz",
      "metadata": {
        "id": "zaCI_Np_G5Zz",
        "outputId": "622d38b4-0f05-4c5d-f1d2-ff2a407e5b54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 12ms/step - loss: 0.1697 - auc: 0.5942 - val_loss: 0.1759 - val_auc: 0.5935 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 11s 14ms/step - loss: 0.1547 - auc: 0.7196 - val_loss: 0.1488 - val_auc: 0.6339 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1385 - auc: 0.7982 - val_loss: 0.1386 - val_auc: 0.6616 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1253 - auc: 0.8421 - val_loss: 0.1380 - val_auc: 0.6899 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1153 - auc: 0.8698 - val_loss: 0.1432 - val_auc: 0.6915 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1088 - auc: 0.8855 - val_loss: 0.1356 - val_auc: 0.7104 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1029 - auc: 0.8989 - val_loss: 0.1411 - val_auc: 0.7132 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0989 - auc: 0.9059 - val_loss: 0.1337 - val_auc: 0.7184 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0964 - auc: 0.9119 - val_loss: 0.1397 - val_auc: 0.7156 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0927 - auc: 0.9184 - val_loss: 0.1483 - val_auc: 0.7192 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0889 - auc: 0.9246 - val_loss: 0.1384 - val_auc: 0.7287 - lr: 9.0484e-04\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0852 - auc: 0.9306 - val_loss: 0.1562 - val_auc: 0.7251 - lr: 8.1873e-04\n",
            "Epoch 13/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0829 - auc: 0.9345 - val_loss: 0.1557 - val_auc: 0.7296 - lr: 7.4082e-04\n",
            "NN with best score of 0.7185240828404615\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1683 - auc: 0.6537 - val_loss: 0.1552 - val_auc: 0.5907 - lr: 7.4082e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1494 - auc: 0.7492 - val_loss: 0.1490 - val_auc: 0.6323 - lr: 7.4082e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1339 - auc: 0.8143 - val_loss: 0.1434 - val_auc: 0.6614 - lr: 7.4082e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1222 - auc: 0.8519 - val_loss: 0.1336 - val_auc: 0.6717 - lr: 7.4082e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1138 - auc: 0.8742 - val_loss: 0.1458 - val_auc: 0.6838 - lr: 7.4082e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1080 - auc: 0.8879 - val_loss: 0.1466 - val_auc: 0.6893 - lr: 7.4082e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1041 - auc: 0.8962 - val_loss: 0.1398 - val_auc: 0.7006 - lr: 7.4082e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1002 - auc: 0.9046 - val_loss: 0.1401 - val_auc: 0.7021 - lr: 7.4082e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0967 - auc: 0.9117 - val_loss: 0.1527 - val_auc: 0.7029 - lr: 7.4082e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1684 - auc: 0.6439 - val_loss: 0.1608 - val_auc: 0.5813 - lr: 7.4082e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 8s 11ms/step - loss: 0.1501 - auc: 0.7442 - val_loss: 0.1456 - val_auc: 0.6602 - lr: 7.4082e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1327 - auc: 0.8181 - val_loss: 0.1368 - val_auc: 0.6843 - lr: 7.4082e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1201 - auc: 0.8572 - val_loss: 0.1345 - val_auc: 0.7044 - lr: 7.4082e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 8s 10ms/step - loss: 0.1121 - auc: 0.8783 - val_loss: 0.1343 - val_auc: 0.7142 - lr: 7.4082e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 8s 10ms/step - loss: 0.1058 - auc: 0.8922 - val_loss: 0.1416 - val_auc: 0.7143 - lr: 7.4082e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 12s 14ms/step - loss: 0.1016 - auc: 0.9014 - val_loss: 0.1422 - val_auc: 0.7315 - lr: 7.4082e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 12ms/step - loss: 0.0976 - auc: 0.9090 - val_loss: 0.1435 - val_auc: 0.7298 - lr: 7.4082e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0948 - auc: 0.9146 - val_loss: 0.1334 - val_auc: 0.7310 - lr: 7.4082e-04\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0927 - auc: 0.9184 - val_loss: 0.1449 - val_auc: 0.7359 - lr: 7.4082e-04\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 8s 11ms/step - loss: 0.0890 - auc: 0.9250 - val_loss: 0.1398 - val_auc: 0.7360 - lr: 6.7032e-04\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 8s 10ms/step - loss: 0.0863 - auc: 0.9294 - val_loss: 0.1415 - val_auc: 0.7343 - lr: 6.0653e-04\n",
            "Epoch 13/100\n",
            "805/805 [==============================] - 8s 10ms/step - loss: 0.0833 - auc: 0.9343 - val_loss: 0.1534 - val_auc: 0.7393 - lr: 5.4881e-04\n",
            "Epoch 14/100\n",
            "805/805 [==============================] - 8s 10ms/step - loss: 0.0807 - auc: 0.9382 - val_loss: 0.1401 - val_auc: 0.7487 - lr: 4.9659e-04\n",
            "NN with best score of 0.7309207839675739\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1677 - auc: 0.6625 - val_loss: 0.1573 - val_auc: 0.5903 - lr: 4.9659e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1469 - auc: 0.7618 - val_loss: 0.1499 - val_auc: 0.6250 - lr: 4.9659e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1293 - auc: 0.8308 - val_loss: 0.1401 - val_auc: 0.6559 - lr: 4.9659e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1176 - auc: 0.8647 - val_loss: 0.1479 - val_auc: 0.6714 - lr: 4.9659e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1095 - auc: 0.8849 - val_loss: 0.1594 - val_auc: 0.6709 - lr: 4.9659e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1031 - auc: 0.8989 - val_loss: 0.1532 - val_auc: 0.6825 - lr: 4.9659e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0986 - auc: 0.9078 - val_loss: 0.1419 - val_auc: 0.6846 - lr: 4.9659e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0957 - auc: 0.9133 - val_loss: 0.1592 - val_auc: 0.6853 - lr: 4.9659e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1668 - auc: 0.6525 - val_loss: 0.1516 - val_auc: 0.6095 - lr: 4.9659e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1461 - auc: 0.7649 - val_loss: 0.1502 - val_auc: 0.6609 - lr: 4.9659e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1283 - auc: 0.8324 - val_loss: 0.1500 - val_auc: 0.6818 - lr: 4.9659e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1168 - auc: 0.8662 - val_loss: 0.1339 - val_auc: 0.7000 - lr: 4.9659e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1089 - auc: 0.8855 - val_loss: 0.1519 - val_auc: 0.7071 - lr: 4.9659e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1028 - auc: 0.8987 - val_loss: 0.1338 - val_auc: 0.7192 - lr: 4.9659e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0981 - auc: 0.9083 - val_loss: 0.1291 - val_auc: 0.7167 - lr: 4.9659e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0950 - auc: 0.9140 - val_loss: 0.1428 - val_auc: 0.7282 - lr: 4.9659e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0925 - auc: 0.9187 - val_loss: 0.1389 - val_auc: 0.7310 - lr: 4.9659e-04\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0901 - auc: 0.9227 - val_loss: 0.1387 - val_auc: 0.7303 - lr: 4.9659e-04\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0867 - auc: 0.9288 - val_loss: 0.1305 - val_auc: 0.7364 - lr: 4.4933e-04\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0832 - auc: 0.9346 - val_loss: 0.1498 - val_auc: 0.7336 - lr: 4.0657e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1673 - auc: 0.6607 - val_loss: 0.1702 - val_auc: 0.6077 - lr: 4.0657e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 8s 10ms/step - loss: 0.1466 - auc: 0.7628 - val_loss: 0.1522 - val_auc: 0.6579 - lr: 4.0657e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 8s 11ms/step - loss: 0.1300 - auc: 0.8279 - val_loss: 0.1319 - val_auc: 0.6776 - lr: 4.0657e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1182 - auc: 0.8633 - val_loss: 0.1318 - val_auc: 0.7020 - lr: 4.0657e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 8s 10ms/step - loss: 0.1096 - auc: 0.8848 - val_loss: 0.1335 - val_auc: 0.7140 - lr: 4.0657e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 8s 10ms/step - loss: 0.1040 - auc: 0.8965 - val_loss: 0.1370 - val_auc: 0.7218 - lr: 4.0657e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 8s 10ms/step - loss: 0.0991 - auc: 0.9071 - val_loss: 0.1280 - val_auc: 0.7240 - lr: 4.0657e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0959 - auc: 0.9130 - val_loss: 0.1340 - val_auc: 0.7324 - lr: 4.0657e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0924 - auc: 0.9192 - val_loss: 0.1311 - val_auc: 0.7373 - lr: 4.0657e-04\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0898 - auc: 0.9240 - val_loss: 0.1338 - val_auc: 0.7414 - lr: 4.0657e-04\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0861 - auc: 0.9301 - val_loss: 0.1415 - val_auc: 0.7416 - lr: 3.6788e-04\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0839 - auc: 0.9334 - val_loss: 0.1418 - val_auc: 0.7480 - lr: 3.3287e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1677 - auc: 0.6512 - val_loss: 0.1847 - val_auc: 0.6014 - lr: 3.3287e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1473 - auc: 0.7601 - val_loss: 0.1259 - val_auc: 0.6548 - lr: 3.3287e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1297 - auc: 0.8288 - val_loss: 0.1336 - val_auc: 0.6761 - lr: 3.3287e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1178 - auc: 0.8641 - val_loss: 0.1553 - val_auc: 0.6869 - lr: 3.3287e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1096 - auc: 0.8850 - val_loss: 0.1299 - val_auc: 0.6953 - lr: 3.3287e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1034 - auc: 0.8987 - val_loss: 0.1475 - val_auc: 0.7062 - lr: 3.3287e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0986 - auc: 0.9087 - val_loss: 0.1412 - val_auc: 0.6989 - lr: 3.3287e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1679 - auc: 0.6475 - val_loss: 0.1698 - val_auc: 0.6029 - lr: 3.3287e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1488 - auc: 0.7517 - val_loss: 0.1605 - val_auc: 0.6461 - lr: 3.3287e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1312 - auc: 0.8235 - val_loss: 0.1298 - val_auc: 0.6838 - lr: 3.3287e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1186 - auc: 0.8617 - val_loss: 0.1494 - val_auc: 0.6942 - lr: 3.3287e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1104 - auc: 0.8824 - val_loss: 0.1344 - val_auc: 0.7060 - lr: 3.3287e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1046 - auc: 0.8956 - val_loss: 0.1351 - val_auc: 0.7174 - lr: 3.3287e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0996 - auc: 0.9055 - val_loss: 0.1333 - val_auc: 0.7134 - lr: 3.3287e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0969 - auc: 0.9108 - val_loss: 0.1413 - val_auc: 0.7211 - lr: 3.3287e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1676 - auc: 0.6521 - val_loss: 0.1672 - val_auc: 0.5928 - lr: 3.3287e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1479 - auc: 0.7575 - val_loss: 0.1362 - val_auc: 0.6531 - lr: 3.3287e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1307 - auc: 0.8265 - val_loss: 0.1386 - val_auc: 0.6811 - lr: 3.3287e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1194 - auc: 0.8596 - val_loss: 0.1364 - val_auc: 0.7023 - lr: 3.3287e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1108 - auc: 0.8818 - val_loss: 0.1291 - val_auc: 0.7228 - lr: 3.3287e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1049 - auc: 0.8950 - val_loss: 0.1234 - val_auc: 0.7208 - lr: 3.3287e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1006 - auc: 0.9040 - val_loss: 0.1286 - val_auc: 0.7244 - lr: 3.3287e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0961 - auc: 0.9123 - val_loss: 0.1272 - val_auc: 0.7336 - lr: 3.3287e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0931 - auc: 0.9188 - val_loss: 0.1314 - val_auc: 0.7372 - lr: 3.3287e-04\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0910 - auc: 0.9217 - val_loss: 0.1312 - val_auc: 0.7399 - lr: 3.3287e-04\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0876 - auc: 0.9274 - val_loss: 0.1327 - val_auc: 0.7344 - lr: 3.0119e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1685 - auc: 0.6490 - val_loss: 0.1459 - val_auc: 0.5981 - lr: 3.0119e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1493 - auc: 0.7516 - val_loss: 0.1689 - val_auc: 0.6590 - lr: 3.0119e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1325 - auc: 0.8201 - val_loss: 0.1491 - val_auc: 0.6750 - lr: 3.0119e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1198 - auc: 0.8593 - val_loss: 0.1371 - val_auc: 0.6928 - lr: 3.0119e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1116 - auc: 0.8801 - val_loss: 0.1271 - val_auc: 0.7080 - lr: 3.0119e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1053 - auc: 0.8943 - val_loss: 0.1386 - val_auc: 0.7107 - lr: 3.0119e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1006 - auc: 0.9041 - val_loss: 0.1371 - val_auc: 0.7165 - lr: 3.0119e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0971 - auc: 0.9112 - val_loss: 0.1324 - val_auc: 0.7181 - lr: 3.0119e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0937 - auc: 0.9173 - val_loss: 0.1347 - val_auc: 0.7212 - lr: 3.0119e-04\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0915 - auc: 0.9205 - val_loss: 0.1492 - val_auc: 0.7304 - lr: 3.0119e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1677 - auc: 0.6434 - val_loss: 0.1742 - val_auc: 0.5948 - lr: 3.0119e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1495 - auc: 0.7492 - val_loss: 0.1465 - val_auc: 0.6430 - lr: 3.0119e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1328 - auc: 0.8196 - val_loss: 0.1432 - val_auc: 0.6686 - lr: 3.0119e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1211 - auc: 0.8548 - val_loss: 0.1316 - val_auc: 0.6841 - lr: 3.0119e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1126 - auc: 0.8777 - val_loss: 0.1367 - val_auc: 0.6886 - lr: 3.0119e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1066 - auc: 0.8915 - val_loss: 0.1427 - val_auc: 0.7061 - lr: 3.0119e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1015 - auc: 0.9026 - val_loss: 0.1337 - val_auc: 0.7095 - lr: 3.0119e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0967 - auc: 0.9116 - val_loss: 0.1379 - val_auc: 0.7108 - lr: 3.0119e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0941 - auc: 0.9166 - val_loss: 0.1415 - val_auc: 0.7150 - lr: 3.0119e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1673 - auc: 0.6550 - val_loss: 0.1506 - val_auc: 0.5804 - lr: 3.0119e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1485 - auc: 0.7539 - val_loss: 0.1490 - val_auc: 0.6249 - lr: 3.0119e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1318 - auc: 0.8220 - val_loss: 0.1400 - val_auc: 0.6631 - lr: 3.0119e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1196 - auc: 0.8599 - val_loss: 0.1392 - val_auc: 0.6833 - lr: 3.0119e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1115 - auc: 0.8799 - val_loss: 0.1372 - val_auc: 0.6867 - lr: 3.0119e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1057 - auc: 0.8927 - val_loss: 0.1405 - val_auc: 0.6986 - lr: 3.0119e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1007 - auc: 0.9037 - val_loss: 0.1316 - val_auc: 0.7009 - lr: 3.0119e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0971 - auc: 0.9112 - val_loss: 0.1348 - val_auc: 0.7065 - lr: 3.0119e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0936 - auc: 0.9171 - val_loss: 0.1351 - val_auc: 0.7146 - lr: 3.0119e-04\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0915 - auc: 0.9208 - val_loss: 0.1430 - val_auc: 0.7141 - lr: 3.0119e-04\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0878 - auc: 0.9275 - val_loss: 0.1353 - val_auc: 0.7208 - lr: 2.7253e-04\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0848 - auc: 0.9322 - val_loss: 0.1392 - val_auc: 0.7221 - lr: 2.4660e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1688 - auc: 0.6488 - val_loss: 0.1577 - val_auc: 0.5893 - lr: 2.4660e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1514 - auc: 0.7390 - val_loss: 0.1661 - val_auc: 0.6346 - lr: 2.4660e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1351 - auc: 0.8112 - val_loss: 0.1530 - val_auc: 0.6672 - lr: 2.4660e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1239 - auc: 0.8474 - val_loss: 0.1486 - val_auc: 0.6868 - lr: 2.4660e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1156 - auc: 0.8702 - val_loss: 0.1337 - val_auc: 0.6998 - lr: 2.4660e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1091 - auc: 0.8858 - val_loss: 0.1283 - val_auc: 0.6964 - lr: 2.4660e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1047 - auc: 0.8956 - val_loss: 0.1315 - val_auc: 0.7026 - lr: 2.4660e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1003 - auc: 0.9049 - val_loss: 0.1307 - val_auc: 0.7141 - lr: 2.4660e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0969 - auc: 0.9113 - val_loss: 0.1349 - val_auc: 0.7220 - lr: 2.4660e-04\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0939 - auc: 0.9168 - val_loss: 0.1428 - val_auc: 0.7184 - lr: 2.4660e-04\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0906 - auc: 0.9229 - val_loss: 0.1332 - val_auc: 0.7227 - lr: 2.2313e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1690 - auc: 0.6453 - val_loss: 0.1748 - val_auc: 0.5729 - lr: 2.2313e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1529 - auc: 0.7301 - val_loss: 0.1589 - val_auc: 0.6302 - lr: 2.2313e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1370 - auc: 0.8020 - val_loss: 0.1573 - val_auc: 0.6547 - lr: 2.2313e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1259 - auc: 0.8409 - val_loss: 0.1406 - val_auc: 0.6685 - lr: 2.2313e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1175 - auc: 0.8650 - val_loss: 0.1351 - val_auc: 0.6767 - lr: 2.2313e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1112 - auc: 0.8809 - val_loss: 0.1463 - val_auc: 0.6868 - lr: 2.2313e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1055 - auc: 0.8938 - val_loss: 0.1324 - val_auc: 0.6956 - lr: 2.2313e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1021 - auc: 0.9011 - val_loss: 0.1397 - val_auc: 0.6883 - lr: 2.2313e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0980 - auc: 0.9089 - val_loss: 0.1496 - val_auc: 0.7031 - lr: 2.2313e-04\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0959 - auc: 0.9131 - val_loss: 0.1434 - val_auc: 0.7091 - lr: 2.2313e-04\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0922 - auc: 0.9198 - val_loss: 0.1452 - val_auc: 0.7082 - lr: 2.0190e-04\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0897 - auc: 0.9244 - val_loss: 0.1369 - val_auc: 0.7101 - lr: 1.8268e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1694 - auc: 0.6423 - val_loss: 0.1858 - val_auc: 0.6127 - lr: 1.8268e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1555 - auc: 0.7152 - val_loss: 0.1740 - val_auc: 0.6449 - lr: 1.8268e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1412 - auc: 0.7847 - val_loss: 0.1605 - val_auc: 0.6663 - lr: 1.8268e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1310 - auc: 0.8246 - val_loss: 0.1375 - val_auc: 0.6737 - lr: 1.8268e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1226 - auc: 0.8501 - val_loss: 0.1544 - val_auc: 0.6918 - lr: 1.8268e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1163 - auc: 0.8672 - val_loss: 0.1370 - val_auc: 0.6934 - lr: 1.8268e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1115 - auc: 0.8796 - val_loss: 0.1471 - val_auc: 0.7019 - lr: 1.8268e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1073 - auc: 0.8898 - val_loss: 0.1320 - val_auc: 0.7097 - lr: 1.8268e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1029 - auc: 0.8986 - val_loss: 0.1465 - val_auc: 0.7124 - lr: 1.8268e-04\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1007 - auc: 0.9035 - val_loss: 0.1391 - val_auc: 0.7184 - lr: 1.8268e-04\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0971 - auc: 0.9111 - val_loss: 0.1514 - val_auc: 0.7150 - lr: 1.6530e-04\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0934 - auc: 0.9176 - val_loss: 0.1444 - val_auc: 0.7150 - lr: 1.4957e-04\n",
            "Epoch 13/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0912 - auc: 0.9217 - val_loss: 0.1469 - val_auc: 0.7214 - lr: 1.3534e-04\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 12ms/step - loss: 0.1697 - auc: 0.6331 - val_loss: 0.1672 - val_auc: 0.5674 - lr: 1.3534e-04\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1580 - auc: 0.7016 - val_loss: 0.1706 - val_auc: 0.6195 - lr: 1.3534e-04\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1457 - auc: 0.7685 - val_loss: 0.1449 - val_auc: 0.6425 - lr: 1.3534e-04\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1365 - auc: 0.8059 - val_loss: 0.1516 - val_auc: 0.6512 - lr: 1.3534e-04\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1283 - auc: 0.8337 - val_loss: 0.1379 - val_auc: 0.6656 - lr: 1.3534e-04\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1223 - auc: 0.8519 - val_loss: 0.1410 - val_auc: 0.6816 - lr: 1.3534e-04\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1173 - auc: 0.8657 - val_loss: 0.1509 - val_auc: 0.6873 - lr: 1.3534e-04\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1129 - auc: 0.8766 - val_loss: 0.1456 - val_auc: 0.6923 - lr: 1.3534e-04\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1096 - auc: 0.8842 - val_loss: 0.1341 - val_auc: 0.6941 - lr: 1.3534e-04\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1057 - auc: 0.8932 - val_loss: 0.1414 - val_auc: 0.7047 - lr: 1.3534e-04\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1024 - auc: 0.9006 - val_loss: 0.1327 - val_auc: 0.7115 - lr: 1.2246e-04\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0997 - auc: 0.9064 - val_loss: 0.1328 - val_auc: 0.7141 - lr: 1.1080e-04\n",
            "Epoch 13/100\n",
            "805/805 [==============================] - 9s 12ms/step - loss: 0.0971 - auc: 0.9110 - val_loss: 0.1337 - val_auc: 0.7208 - lr: 1.0026e-04\n",
            "Epoch 14/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0950 - auc: 0.9150 - val_loss: 0.1379 - val_auc: 0.7189 - lr: 9.0718e-05\n",
            "Epoch 15/100\n",
            "805/805 [==============================] - 9s 12ms/step - loss: 0.0931 - auc: 0.9183 - val_loss: 0.1382 - val_auc: 0.7239 - lr: 8.2085e-05\n",
            "Epoch 16/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.0902 - auc: 0.9238 - val_loss: 0.1395 - val_auc: 0.7237 - lr: 7.4274e-05\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1695 - auc: 0.6366 - val_loss: 0.1644 - val_auc: 0.5930 - lr: 7.4274e-05\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1624 - auc: 0.6712 - val_loss: 0.1655 - val_auc: 0.6109 - lr: 7.4274e-05\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1553 - auc: 0.7184 - val_loss: 0.1577 - val_auc: 0.6353 - lr: 7.4274e-05\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1486 - auc: 0.7532 - val_loss: 0.1379 - val_auc: 0.6419 - lr: 7.4274e-05\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1437 - auc: 0.7764 - val_loss: 0.1775 - val_auc: 0.6489 - lr: 7.4274e-05\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1383 - auc: 0.7992 - val_loss: 0.1642 - val_auc: 0.6602 - lr: 7.4274e-05\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1338 - auc: 0.8149 - val_loss: 0.1476 - val_auc: 0.6685 - lr: 7.4274e-05\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1297 - auc: 0.8288 - val_loss: 0.1661 - val_auc: 0.6702 - lr: 7.4274e-05\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1272 - auc: 0.8374 - val_loss: 0.1534 - val_auc: 0.6798 - lr: 7.4274e-05\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1703 - auc: 0.6125 - val_loss: 0.1653 - val_auc: 0.5791 - lr: 7.4274e-05\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1630 - auc: 0.6695 - val_loss: 0.1671 - val_auc: 0.6020 - lr: 7.4274e-05\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1554 - auc: 0.7200 - val_loss: 0.1557 - val_auc: 0.6193 - lr: 7.4274e-05\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1484 - auc: 0.7566 - val_loss: 0.1757 - val_auc: 0.6373 - lr: 7.4274e-05\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1430 - auc: 0.7805 - val_loss: 0.1668 - val_auc: 0.6393 - lr: 7.4274e-05\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1379 - auc: 0.8007 - val_loss: 0.1512 - val_auc: 0.6522 - lr: 7.4274e-05\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1335 - auc: 0.8163 - val_loss: 0.1589 - val_auc: 0.6633 - lr: 7.4274e-05\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1299 - auc: 0.8292 - val_loss: 0.1378 - val_auc: 0.6696 - lr: 7.4274e-05\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1261 - auc: 0.8406 - val_loss: 0.1442 - val_auc: 0.6752 - lr: 7.4274e-05\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1233 - auc: 0.8496 - val_loss: 0.1537 - val_auc: 0.6826 - lr: 7.4274e-05\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1202 - auc: 0.8584 - val_loss: 0.1388 - val_auc: 0.6863 - lr: 6.7206e-05\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1172 - auc: 0.8662 - val_loss: 0.1417 - val_auc: 0.6906 - lr: 6.0810e-05\n",
            "Epoch 13/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1149 - auc: 0.8726 - val_loss: 0.1389 - val_auc: 0.6985 - lr: 5.5023e-05\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 12ms/step - loss: 0.1702 - auc: 0.6269 - val_loss: 0.1646 - val_auc: 0.5790 - lr: 5.5023e-05\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1644 - auc: 0.6591 - val_loss: 0.1652 - val_auc: 0.5982 - lr: 5.5023e-05\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1593 - auc: 0.6962 - val_loss: 0.1682 - val_auc: 0.6173 - lr: 5.5023e-05\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1538 - auc: 0.7299 - val_loss: 0.1748 - val_auc: 0.6347 - lr: 5.5023e-05\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1487 - auc: 0.7553 - val_loss: 0.1597 - val_auc: 0.6428 - lr: 5.5023e-05\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1449 - auc: 0.7727 - val_loss: 0.1802 - val_auc: 0.6476 - lr: 5.5023e-05\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1410 - auc: 0.7890 - val_loss: 0.1524 - val_auc: 0.6561 - lr: 5.5023e-05\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1373 - auc: 0.8038 - val_loss: 0.1849 - val_auc: 0.6643 - lr: 5.5023e-05\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1338 - auc: 0.8157 - val_loss: 0.1651 - val_auc: 0.6714 - lr: 5.5023e-05\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1313 - auc: 0.8239 - val_loss: 0.1502 - val_auc: 0.6797 - lr: 5.5023e-05\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1280 - auc: 0.8352 - val_loss: 0.1421 - val_auc: 0.6780 - lr: 4.9787e-05\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1258 - auc: 0.8420 - val_loss: 0.1522 - val_auc: 0.6872 - lr: 4.5049e-05\n",
            "Epoch 13/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1224 - auc: 0.8517 - val_loss: 0.1641 - val_auc: 0.6888 - lr: 4.0762e-05\n",
            "Epoch 14/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1208 - auc: 0.8565 - val_loss: 0.1549 - val_auc: 0.6928 - lr: 3.6883e-05\n",
            "Epoch 15/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1192 - auc: 0.8611 - val_loss: 0.1465 - val_auc: 0.6937 - lr: 3.3373e-05\n",
            "Epoch 16/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1178 - auc: 0.8651 - val_loss: 0.1537 - val_auc: 0.6969 - lr: 3.0197e-05\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 12ms/step - loss: 0.1711 - auc: 0.6064 - val_loss: 0.1760 - val_auc: 0.5553 - lr: 3.0197e-05\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1679 - auc: 0.6248 - val_loss: 0.1686 - val_auc: 0.5766 - lr: 3.0197e-05\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1647 - auc: 0.6558 - val_loss: 0.1784 - val_auc: 0.5951 - lr: 3.0197e-05\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1618 - auc: 0.6798 - val_loss: 0.1746 - val_auc: 0.5982 - lr: 3.0197e-05\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1588 - auc: 0.6995 - val_loss: 0.1704 - val_auc: 0.6116 - lr: 3.0197e-05\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1561 - auc: 0.7152 - val_loss: 0.1698 - val_auc: 0.6235 - lr: 3.0197e-05\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1538 - auc: 0.7284 - val_loss: 0.1695 - val_auc: 0.6259 - lr: 3.0197e-05\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1710 - auc: 0.5816 - val_loss: 0.1719 - val_auc: 0.5712 - lr: 3.0197e-05\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1678 - auc: 0.6241 - val_loss: 0.1755 - val_auc: 0.6000 - lr: 3.0197e-05\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1647 - auc: 0.6550 - val_loss: 0.1796 - val_auc: 0.6076 - lr: 3.0197e-05\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 12ms/step - loss: 0.1619 - auc: 0.6788 - val_loss: 0.1739 - val_auc: 0.6225 - lr: 3.0197e-05\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1587 - auc: 0.7000 - val_loss: 0.1742 - val_auc: 0.6360 - lr: 3.0197e-05\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1561 - auc: 0.7162 - val_loss: 0.1639 - val_auc: 0.6399 - lr: 3.0197e-05\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1535 - auc: 0.7310 - val_loss: 0.1595 - val_auc: 0.6449 - lr: 3.0197e-05\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1514 - auc: 0.7422 - val_loss: 0.1762 - val_auc: 0.6553 - lr: 3.0197e-05\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1490 - auc: 0.7545 - val_loss: 0.1550 - val_auc: 0.6583 - lr: 3.0197e-05\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1470 - auc: 0.7624 - val_loss: 0.1827 - val_auc: 0.6611 - lr: 3.0197e-05\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1450 - auc: 0.7723 - val_loss: 0.1546 - val_auc: 0.6650 - lr: 2.7324e-05\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1423 - auc: 0.7832 - val_loss: 0.1647 - val_auc: 0.6734 - lr: 2.4724e-05\n",
            "Epoch 13/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1414 - auc: 0.7873 - val_loss: 0.1682 - val_auc: 0.6676 - lr: 2.2371e-05\n",
            "Epoch 14/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1399 - auc: 0.7941 - val_loss: 0.1576 - val_auc: 0.6754 - lr: 2.0242e-05\n",
            "Epoch 15/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1388 - auc: 0.7973 - val_loss: 0.1630 - val_auc: 0.6777 - lr: 1.8316e-05\n",
            "Epoch 16/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1374 - auc: 0.8026 - val_loss: 0.1638 - val_auc: 0.6791 - lr: 1.6573e-05\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1716 - auc: 0.5837 - val_loss: 0.1686 - val_auc: 0.5596 - lr: 1.6573e-05\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1695 - auc: 0.6031 - val_loss: 0.1725 - val_auc: 0.5682 - lr: 1.6573e-05\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1678 - auc: 0.6245 - val_loss: 0.1747 - val_auc: 0.5705 - lr: 1.6573e-05\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1663 - auc: 0.6407 - val_loss: 0.1743 - val_auc: 0.5791 - lr: 1.6573e-05\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1649 - auc: 0.6537 - val_loss: 0.1766 - val_auc: 0.5817 - lr: 1.6573e-05\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1633 - auc: 0.6665 - val_loss: 0.1690 - val_auc: 0.5845 - lr: 1.6573e-05\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 11ms/step - loss: 0.1716 - auc: 0.5732 - val_loss: 0.1724 - val_auc: 0.5526 - lr: 1.6573e-05\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1694 - auc: 0.6057 - val_loss: 0.1749 - val_auc: 0.5636 - lr: 1.6573e-05\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1676 - auc: 0.6265 - val_loss: 0.1776 - val_auc: 0.5708 - lr: 1.6573e-05\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1662 - auc: 0.6409 - val_loss: 0.1737 - val_auc: 0.5734 - lr: 1.6573e-05\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1647 - auc: 0.6541 - val_loss: 0.1768 - val_auc: 0.5828 - lr: 1.6573e-05\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1634 - auc: 0.6645 - val_loss: 0.1765 - val_auc: 0.5853 - lr: 1.6573e-05\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 12ms/step - loss: 0.1720 - auc: 0.5553 - val_loss: 0.1734 - val_auc: 0.5769 - lr: 1.6573e-05\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1698 - auc: 0.6026 - val_loss: 0.1727 - val_auc: 0.5822 - lr: 1.6573e-05\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1684 - auc: 0.6197 - val_loss: 0.1696 - val_auc: 0.5876 - lr: 1.6573e-05\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1668 - auc: 0.6383 - val_loss: 0.1721 - val_auc: 0.5949 - lr: 1.6573e-05\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1654 - auc: 0.6524 - val_loss: 0.1661 - val_auc: 0.5982 - lr: 1.6573e-05\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1639 - auc: 0.6650 - val_loss: 0.1743 - val_auc: 0.6011 - lr: 1.6573e-05\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1622 - auc: 0.6779 - val_loss: 0.1620 - val_auc: 0.6050 - lr: 1.6573e-05\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1605 - auc: 0.6891 - val_loss: 0.1701 - val_auc: 0.6102 - lr: 1.6573e-05\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1593 - auc: 0.6992 - val_loss: 0.1721 - val_auc: 0.6141 - lr: 1.6573e-05\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1574 - auc: 0.7094 - val_loss: 0.1692 - val_auc: 0.6183 - lr: 1.6573e-05\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1563 - auc: 0.7174 - val_loss: 0.1781 - val_auc: 0.6204 - lr: 1.4996e-05\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1551 - auc: 0.7228 - val_loss: 0.1808 - val_auc: 0.6229 - lr: 1.3569e-05\n",
            "Epoch 1/100\n",
            "805/805 [==============================] - 10s 12ms/step - loss: 0.1721 - auc: 0.5543 - val_loss: 0.1743 - val_auc: 0.5808 - lr: 1.3569e-05\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1703 - auc: 0.5947 - val_loss: 0.1686 - val_auc: 0.5844 - lr: 1.3569e-05\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 9s 12ms/step - loss: 0.1692 - auc: 0.6098 - val_loss: 0.1662 - val_auc: 0.5893 - lr: 1.3569e-05\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1682 - auc: 0.6228 - val_loss: 0.1730 - val_auc: 0.5919 - lr: 1.3569e-05\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1668 - auc: 0.6379 - val_loss: 0.1733 - val_auc: 0.5939 - lr: 1.3569e-05\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 9s 12ms/step - loss: 0.1655 - auc: 0.6503 - val_loss: 0.1682 - val_auc: 0.5985 - lr: 1.3569e-05\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1644 - auc: 0.6587 - val_loss: 0.1667 - val_auc: 0.6002 - lr: 1.3569e-05\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 9s 11ms/step - loss: 0.1633 - auc: 0.6677 - val_loss: 0.1721 - val_auc: 0.6038 - lr: 1.3569e-05\n"
          ]
        }
      ],
      "source": [
        "bestscore=0\n",
        "bestmodel,bestmodel_val_data=None,None\n",
        "scores=[]\n",
        "for train_index, test_index in cv.split(xs,y):\n",
        "      X_train = xs[train_index]\n",
        "      y_train = y[train_index]\n",
        "      X_test = xs[test_index]\n",
        "      y_test = y[test_index]\n",
        "      model= createModel(loss=loss,optimizer=opt, metrics=metrics)\n",
        "      sm=SMOTE()\n",
        "      X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
        "      model.compile(loss=loss,optimizer=opt, metrics=metrics)\n",
        "      schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "      earlystop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
        "      callbacks=[schedule,earlystop]\n",
        "      history=model.fit(X_train_oversampled,y_train_oversampled,batch_size=64, epochs=100,validation_data=(X_test,y_test),verbose=1,callbacks=callbacks)\n",
        "      y_pred = model.predict(X_test,verbose=0)\n",
        "      score=roc_auc_score(y_test,y_pred)\n",
        "      scores.append(score)\n",
        "      if score>bestscore:\n",
        "            bestmodel=model\n",
        "            bestscore,bestmodel_val_data=score,[y_test,X_test]\n",
        "            print(f\"NN with best score of {score}\")\n",
        "score_df_smote[\"NN_Smote\"]=scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "IQJrqaeYHA57",
      "metadata": {
        "id": "IQJrqaeYHA57",
        "outputId": "76cdbeb6-8c44-49bd-cee8-3befdb31e5de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "228/228 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb3894e7650>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "y_test,X_test=bestmodel_val_data\n",
        "view=pd.DataFrame([model.predict(X_test).reshape(-1),y_test.values]).T\n",
        "view.columns=[\"preds\",\"ground_truth\"]\n",
        "view.boxplot(by='ground_truth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "qtZQdhI4KdLE",
      "metadata": {
        "id": "qtZQdhI4KdLE"
      },
      "outputs": [],
      "source": [
        "score_df_smote.to_csv(\"../content/score_nn_smote.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d8v2Phf6KblL",
      "metadata": {
        "id": "d8v2Phf6KblL"
      },
      "outputs": [],
      "source": [
        "score_df_tl=pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ouVb58vcKaFZ",
      "metadata": {
        "id": "ouVb58vcKaFZ",
        "outputId": "cd4ae333-8b96-47dd-9de2-ff9a81751c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0992 - auc: 0.5142 - val_loss: 0.0969 - val_auc: 0.5428 - lr: 1.3569e-05\n",
            "Epoch 2/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0973 - auc: 0.5443 - val_loss: 0.0971 - val_auc: 0.5519 - lr: 1.3569e-05\n",
            "Epoch 3/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0969 - auc: 0.5555 - val_loss: 0.0965 - val_auc: 0.5571 - lr: 1.3569e-05\n",
            "Epoch 4/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0969 - auc: 0.5557 - val_loss: 0.0962 - val_auc: 0.5625 - lr: 1.3569e-05\n",
            "Epoch 5/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.0967 - auc: 0.5602 - val_loss: 0.0962 - val_auc: 0.5661 - lr: 1.3569e-05\n",
            "Epoch 6/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0965 - auc: 0.5661 - val_loss: 0.0966 - val_auc: 0.5723 - lr: 1.3569e-05\n",
            "Epoch 7/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0963 - auc: 0.5728 - val_loss: 0.0962 - val_auc: 0.5755 - lr: 1.3569e-05\n",
            "Epoch 8/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0961 - auc: 0.5795 - val_loss: 0.0960 - val_auc: 0.5810 - lr: 1.3569e-05\n",
            "Epoch 9/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0961 - auc: 0.5791 - val_loss: 0.0962 - val_auc: 0.5827 - lr: 1.3569e-05\n",
            "Epoch 10/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0960 - auc: 0.5806 - val_loss: 0.0955 - val_auc: 0.5884 - lr: 1.3569e-05\n",
            "Epoch 11/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0957 - auc: 0.5903 - val_loss: 0.0959 - val_auc: 0.5898 - lr: 1.2277e-05\n",
            "Epoch 12/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0956 - auc: 0.5920 - val_loss: 0.0955 - val_auc: 0.5933 - lr: 1.1109e-05\n",
            "Epoch 13/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0953 - auc: 0.6003 - val_loss: 0.0958 - val_auc: 0.5929 - lr: 1.0052e-05\n",
            "Epoch 14/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0955 - auc: 0.5965 - val_loss: 0.0958 - val_auc: 0.5931 - lr: 9.0953e-06\n",
            "Epoch 15/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0953 - auc: 0.5997 - val_loss: 0.0951 - val_auc: 0.5996 - lr: 8.2298e-06\n",
            "Epoch 16/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0951 - auc: 0.6084 - val_loss: 0.0956 - val_auc: 0.5973 - lr: 7.4466e-06\n",
            "Epoch 17/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0950 - auc: 0.6083 - val_loss: 0.0956 - val_auc: 0.5985 - lr: 6.7379e-06\n",
            "Epoch 18/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0949 - auc: 0.6132 - val_loss: 0.0952 - val_auc: 0.6006 - lr: 6.0967e-06\n",
            "Epoch 19/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0947 - auc: 0.6205 - val_loss: 0.0952 - val_auc: 0.6005 - lr: 5.5166e-06\n",
            "Epoch 20/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0946 - auc: 0.6191 - val_loss: 0.0951 - val_auc: 0.6023 - lr: 4.9916e-06\n",
            "Epoch 21/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0948 - auc: 0.6121 - val_loss: 0.0951 - val_auc: 0.6026 - lr: 4.5166e-06\n",
            "Epoch 22/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0948 - auc: 0.6131 - val_loss: 0.0951 - val_auc: 0.6030 - lr: 4.0868e-06\n",
            "Epoch 23/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0946 - auc: 0.6174 - val_loss: 0.0949 - val_auc: 0.6055 - lr: 3.6979e-06\n",
            "Epoch 24/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0947 - auc: 0.6167 - val_loss: 0.0952 - val_auc: 0.6062 - lr: 3.3460e-06\n",
            "Epoch 25/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0946 - auc: 0.6204 - val_loss: 0.0949 - val_auc: 0.6060 - lr: 3.0276e-06\n",
            "Epoch 26/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0945 - auc: 0.6263 - val_loss: 0.0950 - val_auc: 0.6047 - lr: 2.7394e-06\n",
            "Epoch 27/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0944 - auc: 0.6250 - val_loss: 0.0949 - val_auc: 0.6055 - lr: 2.4788e-06\n",
            "Epoch 28/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0946 - auc: 0.6188 - val_loss: 0.0949 - val_auc: 0.6071 - lr: 2.2429e-06\n",
            "Epoch 29/100\n",
            "453/453 [==============================] - 6s 13ms/step - loss: 0.0943 - auc: 0.6268 - val_loss: 0.0949 - val_auc: 0.6089 - lr: 2.0294e-06\n",
            "Epoch 30/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0946 - auc: 0.6205 - val_loss: 0.0950 - val_auc: 0.6069 - lr: 1.8363e-06\n",
            "Epoch 31/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0944 - auc: 0.6242 - val_loss: 0.0950 - val_auc: 0.6082 - lr: 1.6616e-06\n",
            "Epoch 32/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0943 - auc: 0.6262 - val_loss: 0.0949 - val_auc: 0.6087 - lr: 1.5034e-06\n",
            "Epoch 33/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0945 - auc: 0.6225 - val_loss: 0.0949 - val_auc: 0.6094 - lr: 1.3604e-06\n",
            "NN with best score of 0.6071761998441172\n",
            "Epoch 1/100\n",
            "453/453 [==============================] - 6s 11ms/step - loss: 0.1097 - auc: 0.5202 - val_loss: 0.0999 - val_auc: 0.4879 - lr: 1.3604e-06\n",
            "Epoch 2/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0999 - auc: 0.4996 - val_loss: 0.0987 - val_auc: 0.4934 - lr: 1.3604e-06\n",
            "Epoch 3/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0990 - auc: 0.5049 - val_loss: 0.0981 - val_auc: 0.4922 - lr: 1.3604e-06\n",
            "Epoch 4/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0989 - auc: 0.5021 - val_loss: 0.0978 - val_auc: 0.4970 - lr: 1.3604e-06\n",
            "Epoch 5/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0985 - auc: 0.5087 - val_loss: 0.0976 - val_auc: 0.4987 - lr: 1.3604e-06\n",
            "Epoch 6/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0984 - auc: 0.5134 - val_loss: 0.0975 - val_auc: 0.5008 - lr: 1.3604e-06\n",
            "Epoch 7/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0982 - auc: 0.5148 - val_loss: 0.0974 - val_auc: 0.5068 - lr: 1.3604e-06\n",
            "Epoch 8/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0983 - auc: 0.5123 - val_loss: 0.0974 - val_auc: 0.5094 - lr: 1.3604e-06\n",
            "Epoch 9/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0981 - auc: 0.5164 - val_loss: 0.0973 - val_auc: 0.5140 - lr: 1.3604e-06\n",
            "Epoch 10/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0979 - auc: 0.5255 - val_loss: 0.0973 - val_auc: 0.5148 - lr: 1.3604e-06\n",
            "Epoch 11/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0980 - auc: 0.5197 - val_loss: 0.0972 - val_auc: 0.5179 - lr: 1.2309e-06\n",
            "Epoch 12/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0979 - auc: 0.5253 - val_loss: 0.0972 - val_auc: 0.5198 - lr: 1.1138e-06\n",
            "Epoch 13/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0978 - auc: 0.5267 - val_loss: 0.0972 - val_auc: 0.5196 - lr: 1.0078e-06\n",
            "Epoch 14/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0978 - auc: 0.5259 - val_loss: 0.0972 - val_auc: 0.5216 - lr: 9.1188e-07\n",
            "Epoch 15/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0979 - auc: 0.5221 - val_loss: 0.0971 - val_auc: 0.5217 - lr: 8.2511e-07\n",
            "Epoch 16/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0977 - auc: 0.5304 - val_loss: 0.0971 - val_auc: 0.5221 - lr: 7.4659e-07\n",
            "Epoch 17/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0978 - auc: 0.5257 - val_loss: 0.0971 - val_auc: 0.5229 - lr: 6.7554e-07\n",
            "Epoch 18/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0978 - auc: 0.5274 - val_loss: 0.0971 - val_auc: 0.5242 - lr: 6.1125e-07\n",
            "Epoch 19/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5330 - val_loss: 0.0971 - val_auc: 0.5249 - lr: 5.5308e-07\n",
            "Epoch 20/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0977 - auc: 0.5296 - val_loss: 0.0971 - val_auc: 0.5246 - lr: 5.0045e-07\n",
            "Epoch 21/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5321 - val_loss: 0.0970 - val_auc: 0.5252 - lr: 4.5283e-07\n",
            "Epoch 22/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0978 - auc: 0.5239 - val_loss: 0.0970 - val_auc: 0.5263 - lr: 4.0974e-07\n",
            "Epoch 23/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5333 - val_loss: 0.0970 - val_auc: 0.5262 - lr: 3.7074e-07\n",
            "Epoch 24/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0977 - auc: 0.5318 - val_loss: 0.0970 - val_auc: 0.5276 - lr: 3.3546e-07\n",
            "Epoch 25/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5349 - val_loss: 0.0970 - val_auc: 0.5272 - lr: 3.0354e-07\n",
            "Epoch 26/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5352 - val_loss: 0.0970 - val_auc: 0.5260 - lr: 2.7465e-07\n",
            "Epoch 27/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0977 - auc: 0.5265 - val_loss: 0.0970 - val_auc: 0.5269 - lr: 2.4852e-07\n",
            "Epoch 28/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5347 - val_loss: 0.0970 - val_auc: 0.5266 - lr: 2.2487e-07\n",
            "Epoch 29/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5367 - val_loss: 0.0970 - val_auc: 0.5269 - lr: 2.0347e-07\n",
            "Epoch 30/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5288 - val_loss: 0.0970 - val_auc: 0.5269 - lr: 1.8411e-07\n",
            "Epoch 31/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5330 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 1.6659e-07\n",
            "Epoch 32/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5297 - val_loss: 0.0970 - val_auc: 0.5268 - lr: 1.5073e-07\n",
            "Epoch 33/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0977 - auc: 0.5295 - val_loss: 0.0970 - val_auc: 0.5265 - lr: 1.3639e-07\n",
            "Epoch 34/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0974 - auc: 0.5394 - val_loss: 0.0970 - val_auc: 0.5268 - lr: 1.2341e-07\n",
            "Epoch 35/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5301 - val_loss: 0.0970 - val_auc: 0.5266 - lr: 1.1167e-07\n",
            "Epoch 36/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5289 - val_loss: 0.0970 - val_auc: 0.5273 - lr: 1.0104e-07\n",
            "Epoch 37/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0977 - auc: 0.5269 - val_loss: 0.0970 - val_auc: 0.5268 - lr: 9.1424e-08\n",
            "Epoch 38/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5366 - val_loss: 0.0970 - val_auc: 0.5266 - lr: 8.2724e-08\n",
            "Epoch 39/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5278 - val_loss: 0.0970 - val_auc: 0.5264 - lr: 7.4852e-08\n",
            "Epoch 40/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5329 - val_loss: 0.0970 - val_auc: 0.5265 - lr: 6.7729e-08\n",
            "Epoch 41/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5306 - val_loss: 0.0970 - val_auc: 0.5265 - lr: 6.1284e-08\n",
            "Epoch 42/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5298 - val_loss: 0.0970 - val_auc: 0.5265 - lr: 5.5452e-08\n",
            "Epoch 43/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0977 - auc: 0.5304 - val_loss: 0.0970 - val_auc: 0.5262 - lr: 5.0175e-08\n",
            "Epoch 44/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5352 - val_loss: 0.0970 - val_auc: 0.5266 - lr: 4.5400e-08\n",
            "Epoch 45/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5351 - val_loss: 0.0970 - val_auc: 0.5263 - lr: 4.1080e-08\n",
            "Epoch 46/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0974 - auc: 0.5381 - val_loss: 0.0970 - val_auc: 0.5263 - lr: 3.7170e-08\n",
            "Epoch 47/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0978 - auc: 0.5258 - val_loss: 0.0970 - val_auc: 0.5263 - lr: 3.3633e-08\n",
            "Epoch 48/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5345 - val_loss: 0.0970 - val_auc: 0.5262 - lr: 3.0433e-08\n",
            "Epoch 49/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0977 - auc: 0.5301 - val_loss: 0.0970 - val_auc: 0.5262 - lr: 2.7536e-08\n",
            "Epoch 50/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5334 - val_loss: 0.0970 - val_auc: 0.5262 - lr: 2.4916e-08\n",
            "Epoch 51/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5315 - val_loss: 0.0970 - val_auc: 0.5264 - lr: 2.2545e-08\n",
            "Epoch 52/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5380 - val_loss: 0.0970 - val_auc: 0.5266 - lr: 2.0400e-08\n",
            "Epoch 53/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5327 - val_loss: 0.0970 - val_auc: 0.5268 - lr: 1.8458e-08\n",
            "Epoch 54/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5357 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 1.6702e-08\n",
            "Epoch 55/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5338 - val_loss: 0.0970 - val_auc: 0.5265 - lr: 1.5112e-08\n",
            "Epoch 56/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0974 - auc: 0.5409 - val_loss: 0.0970 - val_auc: 0.5266 - lr: 1.3674e-08\n",
            "Epoch 57/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5340 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 1.2373e-08\n",
            "Epoch 58/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0977 - auc: 0.5291 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 1.1195e-08\n",
            "Epoch 59/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5319 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 1.0130e-08\n",
            "Epoch 60/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5321 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 9.1661e-09\n",
            "Epoch 61/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0974 - auc: 0.5364 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 8.2938e-09\n",
            "Epoch 62/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5334 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 7.5046e-09\n",
            "Epoch 63/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5356 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 6.7904e-09\n",
            "Epoch 64/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5349 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 6.1442e-09\n",
            "Epoch 65/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0974 - auc: 0.5367 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 5.5595e-09\n",
            "Epoch 66/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.0976 - auc: 0.5315 - val_loss: 0.0970 - val_auc: 0.5267 - lr: 5.0305e-09\n",
            "Epoch 67/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5346 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 4.5518e-09\n",
            "Epoch 68/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0974 - auc: 0.5371 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 4.1186e-09\n",
            "Epoch 69/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.0975 - auc: 0.5344 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 3.7267e-09\n",
            "Epoch 70/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0974 - auc: 0.5385 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 3.3720e-09\n",
            "Epoch 71/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5298 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 3.0511e-09\n",
            "Epoch 72/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5327 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 2.7608e-09\n",
            "Epoch 73/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5337 - val_loss: 0.0970 - val_auc: 0.5269 - lr: 2.4981e-09\n",
            "Epoch 74/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0974 - auc: 0.5369 - val_loss: 0.0970 - val_auc: 0.5269 - lr: 2.2603e-09\n",
            "Epoch 75/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5334 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 2.0452e-09\n",
            "Epoch 76/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5366 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 1.8506e-09\n",
            "Epoch 77/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.0977 - auc: 0.5284 - val_loss: 0.0970 - val_auc: 0.5271 - lr: 1.6745e-09\n",
            "Epoch 78/100\n",
            "453/453 [==============================] - 6s 14ms/step - loss: 0.0974 - auc: 0.5362 - val_loss: 0.0970 - val_auc: 0.5271 - lr: 1.5151e-09\n",
            "Epoch 79/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5325 - val_loss: 0.0970 - val_auc: 0.5271 - lr: 1.3710e-09\n",
            "Epoch 80/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5332 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 1.2405e-09\n",
            "Epoch 81/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5306 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 1.1224e-09\n",
            "Epoch 82/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5356 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 1.0156e-09\n",
            "Epoch 83/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5310 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 9.1898e-10\n",
            "Epoch 84/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5299 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 8.3153e-10\n",
            "Epoch 85/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0974 - auc: 0.5380 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 7.5240e-10\n",
            "Epoch 86/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5362 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 6.8080e-10\n",
            "Epoch 87/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5289 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 6.1601e-10\n",
            "Epoch 88/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5311 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 5.5739e-10\n",
            "Epoch 89/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0976 - auc: 0.5302 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 5.0435e-10\n",
            "Epoch 90/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.0975 - auc: 0.5344 - val_loss: 0.0970 - val_auc: 0.5270 - lr: 4.5635e-10\n",
            "Epoch 1/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.1757 - auc: 0.5056 - val_loss: 0.1759 - val_auc: 0.5027 - lr: 4.5635e-10\n",
            "Epoch 2/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1758 - auc: 0.5077 - val_loss: 0.1759 - val_auc: 0.5028 - lr: 4.5635e-10\n",
            "Epoch 3/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5133 - val_loss: 0.1759 - val_auc: 0.5027 - lr: 4.5635e-10\n",
            "Epoch 4/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1757 - auc: 0.5117 - val_loss: 0.1759 - val_auc: 0.5027 - lr: 4.5635e-10\n",
            "Epoch 5/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1758 - auc: 0.5081 - val_loss: 0.1759 - val_auc: 0.5029 - lr: 4.5635e-10\n",
            "Epoch 6/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5093 - val_loss: 0.1759 - val_auc: 0.5029 - lr: 4.5635e-10\n",
            "Epoch 7/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1757 - auc: 0.5080 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 4.5635e-10\n",
            "Epoch 8/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1757 - auc: 0.5108 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 4.5635e-10\n",
            "Epoch 9/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5070 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 4.5635e-10\n",
            "Epoch 10/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1754 - auc: 0.5119 - val_loss: 0.1758 - val_auc: 0.5029 - lr: 4.5635e-10\n",
            "Epoch 11/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5074 - val_loss: 0.1758 - val_auc: 0.5027 - lr: 4.1293e-10\n",
            "Epoch 12/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5138 - val_loss: 0.1758 - val_auc: 0.5028 - lr: 3.7363e-10\n",
            "Epoch 13/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5102 - val_loss: 0.1758 - val_auc: 0.5029 - lr: 3.3808e-10\n",
            "Epoch 14/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5066 - val_loss: 0.1758 - val_auc: 0.5029 - lr: 3.0590e-10\n",
            "Epoch 15/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1757 - auc: 0.5056 - val_loss: 0.1758 - val_auc: 0.5029 - lr: 2.7679e-10\n",
            "Epoch 16/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1754 - auc: 0.5147 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 2.5045e-10\n",
            "Epoch 17/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5106 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 2.2662e-10\n",
            "Epoch 18/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1754 - auc: 0.5152 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 2.0505e-10\n",
            "Epoch 19/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1754 - auc: 0.5092 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.8554e-10\n",
            "Epoch 20/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5044 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.6788e-10\n",
            "Epoch 21/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5074 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.5191e-10\n",
            "Epoch 22/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5136 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.3745e-10\n",
            "Epoch 23/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5009 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.2437e-10\n",
            "Epoch 24/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5031 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.1254e-10\n",
            "Epoch 25/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1757 - auc: 0.5055 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.0183e-10\n",
            "Epoch 26/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5139 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 9.2136e-11\n",
            "Epoch 27/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1757 - auc: 0.5040 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 8.3368e-11\n",
            "Epoch 28/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5036 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 7.5435e-11\n",
            "Epoch 29/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1753 - auc: 0.5122 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 6.8256e-11\n",
            "Epoch 30/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5082 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 6.1761e-11\n",
            "Epoch 31/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5104 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 5.5883e-11\n",
            "Epoch 32/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1757 - auc: 0.5047 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 5.0565e-11\n",
            "Epoch 33/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1753 - auc: 0.5139 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 4.5753e-11\n",
            "Epoch 34/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1756 - auc: 0.5022 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 4.1399e-11\n",
            "Epoch 35/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5086 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 3.7460e-11\n",
            "Epoch 36/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1753 - auc: 0.5097 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 3.3895e-11\n",
            "Epoch 37/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5172 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 3.0669e-11\n",
            "Epoch 38/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5029 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 2.7751e-11\n",
            "Epoch 39/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1753 - auc: 0.5116 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 2.5110e-11\n",
            "Epoch 40/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5092 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 2.2721e-11\n",
            "Epoch 41/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5108 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 2.0558e-11\n",
            "Epoch 42/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1754 - auc: 0.5049 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 1.8602e-11\n",
            "Epoch 43/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5055 - val_loss: 0.1758 - val_auc: 0.5030 - lr: 1.6832e-11\n",
            "Epoch 44/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1754 - auc: 0.5091 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.5230e-11\n",
            "Epoch 45/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5138 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.3781e-11\n",
            "Epoch 46/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5140 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.2469e-11\n",
            "Epoch 47/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5035 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.1283e-11\n",
            "Epoch 48/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1757 - auc: 0.5081 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.0209e-11\n",
            "Epoch 49/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1756 - auc: 0.5091 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 9.2375e-12\n",
            "Epoch 50/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1758 - auc: 0.5051 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 8.3584e-12\n",
            "Epoch 51/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1757 - auc: 0.5072 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 7.5630e-12\n",
            "Epoch 52/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5048 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 6.8433e-12\n",
            "Epoch 53/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1758 - auc: 0.5078 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 6.1921e-12\n",
            "Epoch 54/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1754 - auc: 0.5137 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 5.6028e-12\n",
            "Epoch 55/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5081 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 5.0696e-12\n",
            "Epoch 56/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5089 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 4.5872e-12\n",
            "Epoch 57/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1757 - auc: 0.5113 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 4.1507e-12\n",
            "Epoch 58/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1756 - auc: 0.5055 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 3.7557e-12\n",
            "Epoch 59/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1754 - auc: 0.5099 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 3.3983e-12\n",
            "Epoch 60/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5067 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 3.0749e-12\n",
            "Epoch 61/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1756 - auc: 0.5054 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 2.7823e-12\n",
            "Epoch 62/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1755 - auc: 0.5101 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 2.5175e-12\n",
            "Epoch 63/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1754 - auc: 0.5095 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 2.2779e-12\n",
            "Epoch 64/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1754 - auc: 0.5100 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 2.0612e-12\n",
            "Epoch 65/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1755 - auc: 0.5103 - val_loss: 0.1758 - val_auc: 0.5031 - lr: 1.8650e-12\n",
            "Epoch 1/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.1568 - auc: 0.4877 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.8650e-12\n",
            "Epoch 2/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1567 - auc: 0.4884 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.8650e-12\n",
            "Epoch 3/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1568 - auc: 0.4874 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.8650e-12\n",
            "Epoch 4/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1568 - auc: 0.4814 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.8650e-12\n",
            "Epoch 5/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1568 - auc: 0.4820 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.8650e-12\n",
            "Epoch 6/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1567 - auc: 0.4851 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.8650e-12\n",
            "Epoch 7/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1568 - auc: 0.4786 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.8650e-12\n",
            "Epoch 8/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1569 - auc: 0.4805 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.8650e-12\n",
            "Epoch 9/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1568 - auc: 0.4845 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.8650e-12\n",
            "Epoch 10/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1568 - auc: 0.4848 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.8650e-12\n",
            "Epoch 11/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1568 - auc: 0.4834 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.6875e-12\n",
            "Epoch 12/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1569 - auc: 0.4817 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.5269e-12\n",
            "Epoch 13/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1569 - auc: 0.4809 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.3816e-12\n",
            "Epoch 14/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1568 - auc: 0.4809 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.2502e-12\n",
            "Epoch 15/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1567 - auc: 0.4856 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.1312e-12\n",
            "Epoch 16/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1570 - auc: 0.4775 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 1.0235e-12\n",
            "Epoch 17/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1567 - auc: 0.4824 - val_loss: 0.1554 - val_auc: 0.4861 - lr: 9.2614e-13\n",
            "Epoch 1/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.1496 - auc: 0.4926 - val_loss: 0.1536 - val_auc: 0.4851 - lr: 9.2614e-13\n",
            "Epoch 2/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1496 - auc: 0.4934 - val_loss: 0.1536 - val_auc: 0.4851 - lr: 9.2614e-13\n",
            "Epoch 3/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1494 - auc: 0.4999 - val_loss: 0.1536 - val_auc: 0.4851 - lr: 9.2614e-13\n",
            "Epoch 4/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1494 - auc: 0.4997 - val_loss: 0.1536 - val_auc: 0.4851 - lr: 9.2614e-13\n",
            "Epoch 5/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1495 - auc: 0.4987 - val_loss: 0.1536 - val_auc: 0.4851 - lr: 9.2614e-13\n",
            "Epoch 6/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1495 - auc: 0.5048 - val_loss: 0.1536 - val_auc: 0.4851 - lr: 9.2614e-13\n",
            "Epoch 1/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.1499 - auc: 0.4815 - val_loss: 0.1525 - val_auc: 0.4809 - lr: 9.2614e-13\n",
            "Epoch 2/100\n",
            "453/453 [==============================] - 5s 11ms/step - loss: 0.1498 - auc: 0.4843 - val_loss: 0.1525 - val_auc: 0.4809 - lr: 9.2614e-13\n",
            "Epoch 3/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1499 - auc: 0.4850 - val_loss: 0.1525 - val_auc: 0.4809 - lr: 9.2614e-13\n",
            "Epoch 4/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.1499 - auc: 0.4832 - val_loss: 0.1525 - val_auc: 0.4809 - lr: 9.2614e-13\n",
            "Epoch 5/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1499 - auc: 0.4856 - val_loss: 0.1525 - val_auc: 0.4809 - lr: 9.2614e-13\n",
            "Epoch 6/100\n",
            "453/453 [==============================] - 5s 12ms/step - loss: 0.1498 - auc: 0.4781 - val_loss: 0.1525 - val_auc: 0.4809 - lr: 9.2614e-13\n",
            "Epoch 1/100\n",
            "453/453 [==============================] - 6s 12ms/step - loss: 0.1431 - auc: 0.4958 - val_loss: 0.1471 - val_auc: 0.5007 - lr: 9.2614e-13\n",
            "Epoch 2/100\n",
            "453/453 [==============================] - 8s 18ms/step - loss: 0.1431 - auc: 0.4979 - val_loss: 0.1471 - val_auc: 0.5007 - lr: 9.2614e-13\n",
            "Epoch 3/100\n",
            "453/453 [==============================] - 8s 18ms/step - loss: 0.1429 - auc: 0.4997 - val_loss: 0.1471 - val_auc: 0.5007 - lr: 9.2614e-13\n",
            "Epoch 4/100\n",
            "453/453 [==============================] - 9s 21ms/step - loss: 0.1429 - auc: 0.5050 - val_loss: 0.1471 - val_auc: 0.5007 - lr: 9.2614e-13\n",
            "Epoch 5/100\n",
            "453/453 [==============================] - 8s 17ms/step - loss: 0.1431 - auc: 0.4981 - val_loss: 0.1471 - val_auc: 0.5007 - lr: 9.2614e-13\n",
            "Epoch 6/100\n",
            "453/453 [==============================] - 8s 17ms/step - loss: 0.1431 - auc: 0.4928 - val_loss: 0.1471 - val_auc: 0.5007 - lr: 9.2614e-13\n",
            "Epoch 1/100\n",
            " 89/453 [====>.........................] - ETA: 11s - loss: 0.1913 - auc: 0.4807"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c09040cdfa46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mX_train_undersampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_undersampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_undersampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_undersampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "bestscore=0\n",
        "bestmodel,bestmodel_val_data=None,None\n",
        "scores=[]\n",
        "for train_index, test_index in cv.split(xs,y):\n",
        "      X_train = xs[train_index]\n",
        "      y_train = y[train_index]\n",
        "      X_test = xs[test_index]\n",
        "      y_test = y[test_index]\n",
        "      model= createModel(loss=loss,optimizer=opt, metrics=metrics)\n",
        "      tl=TomekLinks()\n",
        "      schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "      earlystop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
        "      callbacks=[schedule,earlystop]\n",
        "      X_train_undersampled, y_train_undersampled = tl.fit_resample(X_train, y_train)\n",
        "      model.compile(loss=loss,optimizer=opt, metrics=metrics)\n",
        "      history=model.fit(X_train_undersampled,y_train_undersampled,batch_size=64, epochs=100,validation_data=(X_test,y_test),verbose=1,callbacks=callbacks)\n",
        "      y_pred = model.predict(X_test,verbose=0)\n",
        "      score=roc_auc_score(y_test,y_pred)\n",
        "      scores.append(score)\n",
        "      if score>bestscore:\n",
        "            bestmodel=model\n",
        "            bestscore,bestmodel_val_data=score,[y_test,X_test]\n",
        "            print(f\"NN with best score of {score}\")\n",
        "score_df_tl[\"NN_tl\"]=scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_df_tl[\"NN_tl\"]=scores"
      ],
      "metadata": {
        "id": "qkiPYo3I2sX-"
      },
      "id": "qkiPYo3I2sX-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bVOnO9znK6ZN",
      "metadata": {
        "id": "bVOnO9znK6ZN"
      },
      "outputs": [],
      "source": [
        "y_test,X_test=bestmodel_val_data\n",
        "view=pd.DataFrame([model.predict(X_test).reshape(-1),y_test.values]).T\n",
        "view.columns=[\"preds\",\"ground_truth\"]\n",
        "view.boxplot(by='ground_truth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x3VY5pyWbN3e",
      "metadata": {
        "id": "x3VY5pyWbN3e"
      },
      "outputs": [],
      "source": [
        "score_df_tl.to_csv(\"../content/score_nn_tl.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "525f50cd3df40e9d57f214b2f9bedadf4bf08c85f9f5b3b27734de5b2eace747"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}